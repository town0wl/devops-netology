### ДЗ 13.2

1.  
Локальная директория хороший вариант для передачи временных данных между контейнерами в рамках пода: она находится рядом на той же ноде, что дает быстрое обращение, привязана к поду, не требуется ее отдельно создавать и удалять.  
```
[kube:neto-k8s]$ cat <<EOF | kl apply -f -
> apiVersion: v1
> kind: Pod
> metadata:
>   name: front-and-back
> spec:
>   containers:
>     - name: front
>       image: nginx
>       volumeMounts:
>         - mountPath: "/static"
>           name: shared-data
>     - name: back
>       image: busybox
>       command: ["sleep", "3600"]
>       volumeMounts:
>         - mountPath: "/tmp/to_front"
>           name: shared-data
>   volumes:
>     - name: shared-data
>       emptyDir: {}
> EOF
pod/front-and-back created
```
Теперь на фронт-контейнере доступны файлы с бэка:  
```
[kube:neto-k8s]$ kl exec front-and-back -c front -- ls -lah /static
total 8.0K
drwxrwxrwx 2 root root 4.0K Jul 17 18:46 .
drwxr-xr-x 1 root root 4.0K Jul 17 18:47 ..
[kube:neto-k8s]$ kl exec front-and-back -c back -- ls -lah /tmp/to_front
total 8K     
drwxrwxrwx    2 root     root        4.0K Jul 17 18:46 .
drwxrwxrwt    1 root     root        4.0K Jul 17 18:47 ..
[kube:neto-k8s]$ kl exec front-and-back -c back -- sh -c 'echo "hello from the back" >> /tmp/to_front/hello'
[kube:neto-k8s]$ kl exec front-and-back -c back -- ls -lah /tmp/to_front
total 12K    
drwxrwxrwx    2 root     root        4.0K Jul 17 19:02 .
drwxrwxrwt    1 root     root        4.0K Jul 17 18:47 ..
-rw-r--r--    1 root     root          20 Jul 17 19:02 hello
[kube:neto-k8s]$ kl exec front-and-back -c front -- ls -lah /static
total 12K
drwxrwxrwx 2 root root 4.0K Jul 17 19:02 .
drwxr-xr-x 1 root root 4.0K Jul 17 18:47 ..
-rw-r--r-- 1 root root   20 Jul 17 19:02 hello
[kube:neto-k8s]$ kl exec front-and-back -c front -- cat /static/hello
hello from the back
```

2.  
Провижинер поставился с сервисом и подом:  
```
[kube:neto-k8s]$ kl get sc nfs -o yaml
allowVolumeExpansion: true
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    meta.helm.sh/release-name: nfs-server
    meta.helm.sh/release-namespace: default
  creationTimestamp: "2022-07-17T19:20:23Z"
  labels:
    app: nfs-server-provisioner
    app.kubernetes.io/managed-by: Helm
    chart: nfs-server-provisioner-1.1.3
    heritage: Helm
    release: nfs-server
  name: nfs
  resourceVersion: "164973"
  uid: df93e1b7-7c02-4345-86bc-ad010ff7f956
mountOptions:
- vers=3
provisioner: cluster.local/nfs-server-nfs-server-provisioner
reclaimPolicy: Delete
volumeBindingMode: Immediate
[kube:neto-k8s]$ kl describe service nfs-server-nfs-server-provisioner
Name:              nfs-server-nfs-server-provisioner
Namespace:         default
Labels:            app=nfs-server-provisioner
                   app.kubernetes.io/managed-by=Helm
                   chart=nfs-server-provisioner-1.1.3
                   heritage=Helm
                   release=nfs-server
Annotations:       meta.helm.sh/release-name: nfs-server
                   meta.helm.sh/release-namespace: default
Selector:          app=nfs-server-provisioner,release=nfs-server
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.111.113.30
IPs:               10.111.113.30
Port:              nfs  2049/TCP
TargetPort:        nfs/TCP
Endpoints:         10.234.44.3:2049
Port:              nfs-udp  2049/UDP
TargetPort:        nfs-udp/UDP
Endpoints:         10.234.44.3:2049
Port:              nlockmgr  32803/TCP
TargetPort:        nlockmgr/TCP
Endpoints:         10.234.44.3:32803
Port:              nlockmgr-udp  32803/UDP
TargetPort:        nlockmgr-udp/UDP
Endpoints:         10.234.44.3:32803
Port:              mountd  20048/TCP
TargetPort:        mountd/TCP
Endpoints:         10.234.44.3:20048
Port:              mountd-udp  20048/UDP
TargetPort:        mountd-udp/UDP
Endpoints:         10.234.44.3:20048
Port:              rquotad  875/TCP
TargetPort:        rquotad/TCP
Endpoints:         10.234.44.3:875
Port:              rquotad-udp  875/UDP
TargetPort:        rquotad-udp/UDP
Endpoints:         10.234.44.3:875
Port:              rpcbind  111/TCP
TargetPort:        rpcbind/TCP
Endpoints:         10.234.44.3:111
Port:              rpcbind-udp  111/UDP
TargetPort:        rpcbind-udp/UDP
Endpoints:         10.234.44.3:111
Port:              statd  662/TCP
TargetPort:        statd/TCP
Endpoints:         10.234.44.3:662
Port:              statd-udp  662/UDP
TargetPort:        statd-udp/UDP
Endpoints:         10.234.44.3:662
Session Affinity:  None
Events:            <none>
[kube:neto-k8s]$ kl get po -o wide
NAME                                  READY   STATUS    RESTARTS        AGE     IP             NODE    NOMINATED NODE   READINESS GATES
example                               1/1     Running   0               4m55s   10.234.44.4    node2   <none>           <none>
front-and-back                        2/2     Running   1 (7m33s ago)   67m     10.234.154.1   node1   <none>           <none>
nfs-server-nfs-server-provisioner-0   1/1     Running   0               34m     10.234.44.3    node2   <none>           <none>
```
Создание фронта и бэка с PVC:  
```
[kube:neto-k8s]$ cat <<EOF | kl apply -f -
> apiVersion: apps/v1
> kind: Deployment
> metadata:
>   labels:
>     app: nginx
>   name: frontend
> spec:
>   replicas: 2
>   selector:
>     matchLabels:
>       app: nginx
>   template:
>     metadata:
>       labels:
>         app: nginx
>     spec:
>       containers:
>       - image: nginx:1.20
>         imagePullPolicy: IfNotPresent
>         name: nginx
>         volumeMounts:
>           - mountPath: "/static"
>             name: shared-data
>       - image: praqma/network-multitool:alpine-extra
>         imagePullPolicy: IfNotPresent
>         name: multitool
>         env:
>           - name: HTTP_PORT
>             value: "8080"
>       volumes:
>         - name: shared-data
>           persistentVolumeClaim:
>             claimName: common-nfs
> ---
> apiVersion: apps/v1
> kind: Deployment
> metadata:
>   labels:
>     app: multitool
>   name: backend
>   namespace: default
> spec:
>   replicas: 2
>   selector:
>     matchLabels:
>       app: multitool
>   template:
>     metadata:
>       labels:
>         app: multitool
>     spec:
>       containers:
>       - image: praqma/network-multitool:alpine-extra
>         imagePullPolicy: IfNotPresent
>         name: network-multitool
>         resources:
>           limits:
>             cpu: 200m
>             memory: 312Mi
>           requests:
>             cpu: 100m
>             memory: 156Mi
>         volumeMounts:
>           - mountPath: "/static"
>             name: shared-data
>       volumes:
>         - name: shared-data
>           persistentVolumeClaim:
>             claimName: common-nfs
> ---
> apiVersion: v1
> kind: PersistentVolumeClaim
> metadata:
>   name: common-nfs
> spec:
>   storageClassName: "nfs"
>   accessModes:
>     - ReadWriteMany
>   resources:
>     requests:
>       storage: 100Mi
> EOF
deployment.apps/frontend created
deployment.apps/backend created
persistentvolumeclaim/common-nfs created
```
Все контейнеры с маунтами имеют доступ к общему PV:
```
[kube:neto-k8s]$ kl get po -o wide
NAME                                  READY   STATUS    RESTARTS      AGE   IP             NODE    NOMINATED NODE   READINESS GATES
backend-6b869c94f8-wmtz9              1/1     Running   0             42s   10.234.154.5   node1   <none>           <none>
backend-6b869c94f8-x5kpj              1/1     Running   0             42s   10.234.44.8    node2   <none>           <none>
example                               1/1     Running   0             35m   10.234.44.4    node2   <none>           <none>
front-and-back                        2/2     Running   1 (38m ago)   98m   10.234.154.1   node1   <none>           <none>
frontend-96df8dd87-44vml              2/2     Running   0             42s   10.234.44.7    node2   <none>           <none>
frontend-96df8dd87-wcdsb              2/2     Running   0             42s   10.234.154.4   node1   <none>           <none>
nfs-server-nfs-server-provisioner-0   1/1     Running   0             65m   10.234.44.3    node2   <none>           <none>
[kube:neto-k8s]$ kl exec frontend-96df8dd87-44vml -c nginx -- ls -lah /static
total 8.0K
drwxrwsrwx 2 root root 4.0K Jul 17 20:24 . 
drwxr-xr-x 1 root root 4.0K Jul 17 20:24 ..
[kube:neto-k8s]$ kl exec frontend-96df8dd87-44vml -c nginx -- sh -c 'echo "from frontend-96df8dd87-44vml" >> /static/log_file'
[kube:neto-k8s]$ kl exec backend-6b869c94f8-x5kpj -- cat /static/log_file
from frontend-96df8dd87-44vml
[kube:neto-k8s]$ kl exec backend-6b869c94f8-x5kpj -- sh -c 'echo "2: from backend-6b869c94f8-x5kpj" >> /static/log_file'
[kube:neto-k8s]$ kl exec frontend-96df8dd87-wcdsb -c nginx -- cat /static/log_file
from frontend-96df8dd87-44vml
2: from backend-6b869c94f8-x5kpj
[kube:neto-k8s]$ kl exec frontend-96df8dd87-wcdsb -c nginx -- sh -c 'echo "3: from frontend-96df8dd87-wcdsb" >> /static/log_file'
[kube:neto-k8s]$ kl exec backend-6b869c94f8-wmtz9 -- cat /static/log_file
from frontend-96df8dd87-44vml
2: from backend-6b869c94f8-x5kpj
3: from frontend-96df8dd87-wcdsb
[kube:neto-k8s]$ kl exec backend-6b869c94f8-wmtz9 -- sh -c 'echo "4: from backend-6b869c94f8-wmtz9" >> /static/log_file' 
[kube:neto-k8s]$ kl exec frontend-96df8dd87-44vml -c nginx -- cat /static/log_file
from frontend-96df8dd87-44vml
2: from backend-6b869c94f8-x5kpj
3: from frontend-96df8dd87-wcdsb
4: from backend-6b869c94f8-wmtz9
```
